{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ded558-b8d9-4438-9c1b-14942d6b2b25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c284c2-61bd-4495-818d-d3ec9524526a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -r /opt/workspace/checkpoint/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ed6537-ccc1-423d-bba6-18a0d6069a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-43fa8264-32ac-4d80-8222-72732b27bd98;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.1 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      ":: resolution report :: resolve 765ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-43fa8264-32ac-4d80-8222-72732b27bd98\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/14ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 21:09:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Notebook\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3038c002-21d3-49c7-a1ae-1a3620b646ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_ride_from_kafka_message(df_raw, schema):\n",
    "    \"\"\" take a Spark Streaming df and parse value col based on <schema>, return streaming df cols in schema \"\"\"\n",
    "    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    df = df_raw.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "    # split attributes to nested array in one Column\n",
    "    col = F.split(df['value'], ', ')\n",
    "\n",
    "    # expand col to multiple top-level columns\n",
    "    for idx, field in enumerate(schema):\n",
    "        df = df.withColumn(field.name, col.getItem(idx).cast(field.dataType))\n",
    "    return df.select([field.name for field in schema])\n",
    "\n",
    "# %%\n",
    "def sink_console(df, output_mode: str = 'complete', processing_time: str = '5 seconds'):\n",
    "    write_query = df.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .trigger(processingTime=processing_time) \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .start()\n",
    "    return write_query # pyspark.sql.streaming.StreamingQuery\n",
    "\n",
    "# %%\n",
    "def sink_memory(df, query_name, query_template):\n",
    "    write_query = df \\\n",
    "        .writeStream \\\n",
    "        .queryName(query_name) \\\n",
    "        .format('memory') \\\n",
    "        .start()\n",
    "    query_str = query_template.format(table_name=query_name)\n",
    "    query_results = spark.sql(query_str)\n",
    "    return write_query, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb0d75be-f8a4-4a9d-a779-56c3d7021acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kafka_green_raw = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n",
    "    .option(\"subscribe\", \"green_rides_csv\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .load()\n",
    "\n",
    "# %%\n",
    "df_kafka_green_raw.printSchema()\n",
    "\n",
    "# %%\n",
    "GREEN_RIDES_SCHEMA = T.StructType(\n",
    "    [\n",
    "     T.StructField(\"vendor_id\", T.IntegerType()),\n",
    "     T.StructField('pickup_datetime', T.TimestampType()),\n",
    "     T.StructField('dropoff_datetime', T.TimestampType()),\n",
    "     T.StructField(\"store_and_fwd_flag\", T.StringType()),\n",
    "     T.StructField(\"RatecodeID\", T.IntegerType()),\n",
    "     T.StructField(\"PULocationID\", T.IntegerType()),\n",
    "     T.StructField(\"DOLocationID\", T.IntegerType()),\n",
    "     T.StructField(\"passenger_count\", T.IntegerType())\n",
    "     ])\n",
    "\n",
    "# %%\n",
    "df_green_rides = parse_ride_from_kafka_message(df_raw=df_kafka_green_raw, schema=GREEN_RIDES_SCHEMA)\n",
    "df_green_rides.printSchema()\n",
    "\n",
    "# %%\n",
    "# query_name = 'green_PUlocationID_popularity'\n",
    "# query_template = 'select PUlocationID,count(*) as count_rides from {table_name} group by PUlocationID order by count_rides desc'\n",
    "# write_query, df_green_PUlocationID_popularity = sink_memory(df=df_green_rides, query_name=query_name, query_template=query_template)\n",
    "# print(type(write_query)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdf2d577-bc90-4347-985a-983ddff32a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kafka_fhv_raw = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n",
    "    .option(\"subscribe\", \"fhv_rides_csv\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .load()\n",
    "\n",
    "# %%\n",
    "df_kafka_fhv_raw.printSchema()\n",
    "\n",
    "# %%\n",
    "FHV_RIDES_SCHEMA =T.StructType(\n",
    "    [\n",
    "     T.StructField(\"dispatching_base_num\", T.StringType()),\n",
    "     T.StructField('pickup_datetime', T.TimestampType()),\n",
    "     T.StructField('dropoff_datetime', T.TimestampType()),\n",
    "     T.StructField(\"PUlocationID\", T.IntegerType()),\n",
    "     T.StructField(\"DOlocationID\", T.IntegerType()),\n",
    "     T.StructField(\"SR_Flag\", T.StringType()),\n",
    "     T.StructField(\"Affiliated_base_number\", T.StringType()),\n",
    "     ])\n",
    "\n",
    "# %%\n",
    "df_fhv_rides = parse_ride_from_kafka_message(df_raw=df_kafka_fhv_raw, schema=FHV_RIDES_SCHEMA)\n",
    "df_fhv_rides.printSchema()\n",
    "\n",
    "# %%\n",
    "# query_name = 'fhv_PUlocationID_popularity'\n",
    "# query_template = 'select PUlocationID,count(*) as count_rides from {table_name} group by PUlocationID order by count_rides desc'\n",
    "# write_query, df_fhv_PUlocationID_popularity = sink_memory(df=df_fhv_rides, query_name=query_name, query_template=query_template)\n",
    "# print(type(write_query)) # pyspark.sql.streaming.StreamingQuery\n",
    "# write_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83393e43-948c-48e0-87c6-4e60cd2ae8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataframe_to_kafka_sink(df, value_columns, key_column=None):\n",
    "    columns = df.columns\n",
    "    df = df.withColumn(\"value\", F.concat_ws(', ',*value_columns))    \n",
    "    if key_column:\n",
    "        df = df.withColumnRenamed(key_column,\"key\")\n",
    "        df = df.withColumn(\"key\",df.key.cast('string'))\n",
    "    return df.select(['key', 'value'])\n",
    "    \n",
    "def sink_kafka(df,checkpointname ,topic, output_mode='append'):\n",
    "    write_query = df.writeStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .option(\"topic\", topic) \\\n",
    "        .option(\"checkpointLocation\", f\"checkpoint{checkpointname}\") \\\n",
    "        .start()\n",
    "    return write_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad538149-5192-4109-a755-45b64d76153d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOPIC_RIDES_ALL = \"rides_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "616c5195-e3c6-459e-810e-6bbb6754d32e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_green_messages = prepare_dataframe_to_kafka_sink(df=df_green_rides,\n",
    "                                                      value_columns=['PUlocationID','DOLocationID'], key_column='PULocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "385217a0-ccb9-4ca9-94a4-a2b86cb10eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 21:19:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/03/13 21:19:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-8a64f53d-3304-43ef-8f90-6ecf53d5e60e-400859102-executor-5, groupId=spark-kafka-source-8a64f53d-3304-43ef-8f90-6ecf53d5e60e-400859102-executor] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/03/13 21:19:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-8a64f53d-3304-43ef-8f90-6ecf53d5e60e-400859102-executor-5, groupId=spark-kafka-source-8a64f53d-3304-43ef-8f90-6ecf53d5e60e-400859102-executor] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kafka_sink_green_query = sink_kafka(df=df_green_messages,checkpointname='green', topic=TOPIC_RIDES_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a2f8abc-fe7e-408c-a5bb-d1429fb0a67b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_sink_green_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d3fb829-f229-4aca-a49f-8ad2ba619d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_fhv_messages = prepare_dataframe_to_kafka_sink(df=df_fhv_rides,\n",
    "                                                      value_columns=['PUlocationID','DOLocationID'], key_column='PULocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "547f5a66-550b-47d8-b39e-aaccbbef096b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 21:19:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kafka_sink_fhv_query = sink_kafka(df=df_fhv_messages,checkpointname='fhv', topic=TOPIC_RIDES_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f68bd97-031e-4ad9-a08b-b1f345478627",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_sink_fhv_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7142a46f-2036-4723-8610-926cc7d717fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1309d300-cae0-4df0-8ff8-752950fd7816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kafka_all_raw = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n",
    "    .option(\"subscribe\", \"rides_all\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpointrides\") \\\n",
    "    .load()\n",
    "\n",
    "# %%\n",
    "df_kafka_all_raw.printSchema()\n",
    "\n",
    "# %%\n",
    "all_RIDES_SCHEMA =T.StructType(\n",
    "    [\n",
    "     T.StructField(\"PUlocationID\", T.IntegerType()),\n",
    "     T.StructField(\"DOlocationID\", T.IntegerType())\n",
    "     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3779259a-552e-4d3d-9b8a-486966993eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "df_all_rides = parse_ride_from_kafka_message(df_raw=df_kafka_all_raw, schema=all_RIDES_SCHEMA)\n",
    "df_all_rides.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69fa685a-fa1c-4aab-8744-bd3955b11181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 21:20:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c91a2564-4f25-4871-8676-4ee587592dd5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/03/13 21:20:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/03/13 21:20:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e3b9747f-e48d-4bbd-abf2-341e0d13cc38-1062345481-driver-0-8, groupId=spark-kafka-source-e3b9747f-e48d-4bbd-abf2-341e0d13cc38-1062345481-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/03/13 21:20:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e3b9747f-e48d-4bbd-abf2-341e0d13cc38-1062345481-driver-0-8, groupId=spark-kafka-source-e3b9747f-e48d-4bbd-abf2-341e0d13cc38-1062345481-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "query_name = 'PUlocationID_popularity'\n",
    "query_template = 'select PUlocationID,count(*) as count_rides from {table_name} group by PUlocationID order by count_rides desc'\n",
    "write_query, df_PUlocationID_popularity = sink_memory(df=df_all_rides, query_name=query_name, query_template=query_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1f51f21-a752-4304-b02b-e56ffd1831d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.streaming.StreamingQuery'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(write_query)) # pyspark.sql.streaming.StreamingQuery\n",
    "write_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae16d474-feab-4e8e-b910-93980040661f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|PUlocationID|count_rides|\n",
      "+------------+-----------+\n",
      "|        null|        116|\n",
      "|         265|         54|\n",
      "|          49|         30|\n",
      "|         256|         27|\n",
      "|          85|         24|\n",
      "|         255|         24|\n",
      "|         146|         18|\n",
      "|         129|         18|\n",
      "|          76|         15|\n",
      "|          82|         15|\n",
      "|         264|         15|\n",
      "|         189|         15|\n",
      "|          97|         15|\n",
      "|          25|         15|\n",
      "|         223|          9|\n",
      "|           7|          9|\n",
      "|          80|          9|\n",
      "|          74|          9|\n",
      "|          71|          9|\n",
      "|          66|          9|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_PUlocationID_popularity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de9510e1-4c96-4003-908d-2a2205d5e416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a86edb-8d92-459f-a747-964c0cc62725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
